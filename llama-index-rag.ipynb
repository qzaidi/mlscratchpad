{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb87197-c625-4408-bce8-5c98334463c5",
   "metadata": {},
   "source": [
    "# RAG with llama index\n",
    "\n",
    "Since llama index seems to have better frontend tooling with create-llama etc, here we attempt to redo a rag but with llama-index. We will reuse the embeddings that we created in [this notebook](./rag-llama-cloud-docs.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f541a184-2eca-4864-b893-fa427e35c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index chromadb --quiet\n",
    "%pip install llama-index-vector-stores-chroma --quiet\n",
    "%pip install llama-index-embeddings-ollama --quiet\n",
    "%pip install llama-index-llms-ollama --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7bdf0-dce1-4cfc-b0cc-e30920a7aa5f",
   "metadata": {},
   "source": [
    "Here we turn debug logging on, so as to better understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae62b36c-3bec-4d2d-9c8f-db7827e56080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3912c4a-bc24-4a46-8ce3-9c1555558252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "473\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma')\n",
    "col = chroma_client.get_collection('snappcloud')\n",
    "print(col.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0e61b7-e9ee-4bee-ac9e-c852d77565d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "base_url=\"http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io/\"\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:v1.5\",\n",
    "    base_url=base_url,\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "\n",
    "llm = Ollama(model=\"llama3\", base_url = base_url, request_timeout=120.0) #json_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec66a328-2cb0-4e26-ab13-8fd62d0f6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c4136e-d3fe-4c48-bce4-7bfe20c58eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=col)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, storage_context=storage_context, embed_model=embed_model\n",
    "#)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store,embed_model=ollama_embedding,storage_context=storage_context)\n",
    "query_engine = index.as_query_engine(llm=llm,response_mode=\"tree_summarize\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e898d2-971b-41cc-9c81-2878018d3735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io//api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io//api/chat \"HTTP/1.1 200 OK\"\n",
      "According to the provided context information, the external IPs for SnappCloud that you need to whitelist are:\n",
      "\n",
      "* 172.16.88.0/22 (for Afranet)\n",
      "* 172.21.88.0/22 (for Asiatech)\n",
      "\n",
      "These IPs can be accessed via VPN and are routable from the VPN.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"What are the external IPs for snappcloud that i need to whitelist?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789befd6-3b47-4826-9126-285ec5689955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io//api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io//api/chat \"HTTP/1.1 200 OK\"\n",
      "You need to use the following address for the Jaeger agent on SnappCloud: \"jaeger - collector.service.consul: 14250\".\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What address should I use for jaeger agent on snappcloud?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f4dc74-1509-438f-a7cd-fc7f0b77c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io//api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io//api/chat \"HTTP/1.1 200 OK\"\n",
      "To increase the quota for your project, you can run the command `oc edit quota` with the name of your project. This will allow the admin of each project to edit its project quota and update it to meet your needs. Please note that the quota of your projects is counted towards the team quota, so be careful not to exceed the team quota limit.\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine.query(\"I need to increase the quota for my project, how can I do this?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3220b-ee7c-419d-9186-5c433bb20b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8718d-776f-4424-98ea-b973ca5786d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
