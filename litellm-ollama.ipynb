{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f66359-dea8-476d-8066-0b87ec3bf644",
   "metadata": {},
   "source": [
    "## LiteLLM\n",
    "\n",
    "LiteLLM is a proxy, which offers an openai like/unified API over a lot of LLMs. In this notebook lets experiment with it\n",
    "\n",
    "First, you need to pull the docker image - and we are using the ollama one as we are only interested in that.\n",
    "\n",
    "docker pull litellm/ollama"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa8642f4-8ab2-465e-ab4a-ea4d58c6f265",
   "metadata": {},
   "source": [
    "Now, run it \n",
    "\n",
    "docker run --name ollama litellm/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8f26fb-fee8-4eca-a7f2-9d2625ff84f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting litellm\n",
      "  Downloading litellm-1.37.9-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: aiohttp in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (3.9.3)\n",
      "Requirement already satisfied: click in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (7.0.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (3.1.3)\n",
      "Collecting openai>=1.0.0 (from litellm)\n",
      "  Downloading openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (0.21.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (2.31.0)\n",
      "Collecting tiktoken>=0.4.0 (from litellm)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tokenizers in /home/qasim/anaconda3/lib/python3.11/site-packages (from litellm) (0.19.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/qasim/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (1.10.12)\n",
      "Requirement already satisfied: sniffio in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/qasim/anaconda3/lib/python3.11/site-packages (from openai>=1.0.0->litellm) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/qasim/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/qasim/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/qasim/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/qasim/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/qasim/anaconda3/lib/python3.11/site-packages (from tiktoken>=0.4.0->litellm) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/qasim/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/qasim/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/qasim/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm) (1.9.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/qasim/anaconda3/lib/python3.11/site-packages (from tokenizers->litellm) (0.23.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/qasim/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->litellm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/qasim/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/qasim/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/qasim/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/qasim/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/qasim/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.1)\n",
      "Downloading litellm-1.37.9-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m241.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m504.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m80.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken, openai, litellm\n",
      "Successfully installed litellm-1.37.9 openai-1.30.1 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d6c3f5a-6488-4591-87c9-3925b9b25408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just an AI, I don't have a physical presence, so I don't have access to real-time weather information. However, I can generate a random weather report for you!\n",
      "\n",
      "According to my fictional weather forecast...\n",
      "\n",
      "It's a lovely day\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(\n",
    "  model=\"ollama/llama3\",\n",
    "  api_base=\"http://localhost:11434\",\n",
    "  messages=[\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"what's the weather like?\"\n",
    "      }\n",
    "  ],\n",
    "  max_tokens=50,\n",
    ")\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b3923-ff4a-4525-9499-776c39237899",
   "metadata": {},
   "source": [
    "Its also possible to use the openai interface, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fdc02-35f7-485c-9e97-b7ed644ca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "api_base = f\"http://0.0.0.0:4000\" # base url for server\n",
    "\n",
    "openai.api_base = api_base\n",
    "openai.api_key = \"temp-key\"\n",
    "print(openai.api_base)\n",
    "\n",
    "\n",
    "print(f'LiteLLM: response from proxy with streaming')\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"ollama/llama3\", \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"this is a test request, acknowledge that you got it\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(f'LiteLLM: streaming response from proxy {chunk}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
