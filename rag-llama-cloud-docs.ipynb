{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46d3b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install ollama sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49c9773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install langchain langchain_community chromadb unstructured qdrant_client helper_utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4925d",
   "metadata": {},
   "source": [
    "Ok, now lets do our RAG. Here we can use the cloud resources. If you want to use a local ollama server, you can create a client to localhost:11434 or alternatively just set client = ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fee29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollamaBase = 'http://ollama-alibo-gpu-testing.apps.private.okd4.teh-2.snappcloud.io/'\n",
    "\n",
    "import ollama\n",
    "\n",
    "client = ollama.Client(ollamaBase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e46273a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110af750-92dd-405d-88f0-470fbfd0a696",
   "metadata": {},
   "source": [
    "It is expected that the docs directory from snappcloud gitlab documentation is copied and available in the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87789636-20cf-4779-9346-2759b09dbc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['security-compliance:latest', 'llama3:70b', 'llama3:latest', 'llama3:instruct']\n"
     ]
    }
   ],
   "source": [
    "def mname(x):\n",
    "    return x['name']\n",
    "    \n",
    "\n",
    "result = client.list()\n",
    "print(list(map(mname,result['models'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11c64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./docs',glob=\"**/*.md\",loader_cls=UnstructuredMarkdownLoader)\n",
    "embeddingModel = 'mxbai-embed-large'\n",
    "llmModel = 'llama3'\n",
    "systemPrompt = \"You are a helpful assistant that answers questions using the context provided. Cite the relevant documents everytime you provide an answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8d06cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['docs/vpn-access.md', 'docs/servicedesk.md', 'docs/overview.md', 'docs/support.md', 'docs/terms.md', 'docs/reference/api-documentation.md', 'docs/reference/cli-documentation.md', 'docs/storage/storage-volumes.md', 'docs/storage/volume-snapshots.md', 'docs/storage/object-store/aws-s3-sdk.md']\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = loader.load()\n",
    "print(list(map(lambda x: x.metadata['source'], docs[0:10])))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419fa87a",
   "metadata": {},
   "source": [
    "We can skip the splitting etc, as the documents are already split, but if you wanted to, you can use the following code\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2675c81",
   "metadata": {},
   "source": [
    "Now we use our embedding model to create the embeddings and store them in chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80e6e577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62 µs, sys: 0 ns, total: 62 µs\n",
      "Wall time: 65.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=embeddingModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f415506-93dc-4728-8bf0-82f9c6ed14ba",
   "metadata": {},
   "source": [
    "We should save the embeddings in a vector db, such as chroma or qdrant. Below are both the examples but you can change the one you want to use as code, and the other field can be set as markdown text.\n",
    "We are using the mxbai-embed-large model, whose context window is 512, so we must ensure all of our docs are of smaller size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e8e22b2-3361-47bf-982d-3b2afa4b5bda",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m token_split_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m----> 5\u001b[0m     token_split_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m token_splitter\u001b[38;5;241m.\u001b[39msplit_text(text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_wrap(token_split_texts[\u001b[38;5;241m10\u001b[39m]))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(token_split_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/text_splitter.py:775\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode(text)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    768\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(\n\u001b[1;32m    769\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_overlap,\n\u001b[1;32m    770\u001b[0m     tokens_per_chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_per_chunk,\n\u001b[1;32m    771\u001b[0m     decode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode,\n\u001b[1;32m    772\u001b[0m     encode\u001b[38;5;241m=\u001b[39mencode_strip_start_and_stop_token_ids,\n\u001b[1;32m    773\u001b[0m )\n\u001b[0;32m--> 775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m split_text_on_tokens(text\u001b[38;5;241m=\u001b[39mtext, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/text_splitter.py:659\u001b[0m, in \u001b[0;36msplit_text_on_tokens\u001b[0;34m(text, tokenizer)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\u001b[39;00m\n\u001b[1;32m    658\u001b[0m splits: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 659\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m    660\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    661\u001b[0m cur_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokens_per_chunk, \u001b[38;5;28mlen\u001b[39m(input_ids))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/text_splitter.py:766\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.split_text.<locals>.encode_strip_start_and_stop_token_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_strip_start_and_stop_token_ids\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode(text)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/text_splitter.py:783\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter._encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 783\u001b[0m     token_ids_with_start_and_end_token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m    784\u001b[0m         text,\n\u001b[1;32m    785\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_length_equal_32_bit_integer,\n\u001b[1;32m    786\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_not_truncate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    787\u001b[0m     )\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_ids_with_start_and_end_token_ids\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2608\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2631\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2642\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2644\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2645\u001b[0m         text,\n\u001b[1;32m   2646\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2647\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2648\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2649\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2650\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2651\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2652\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2653\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2654\u001b[0m     )\n\u001b[1;32m   2656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3044\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3045\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3050\u001b[0m )\n\u001b[0;32m-> 3052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   3053\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3054\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   3055\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   3056\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   3057\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   3058\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3059\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   3060\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   3061\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3062\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3063\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3064\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3065\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3066\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3067\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3068\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3069\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3070\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3071\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    577\u001b[0m         batched_input,\n\u001b[1;32m    578\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    579\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    580\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    581\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    582\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    583\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    584\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    585\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    586\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    587\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    588\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    589\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    590\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    591\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    592\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    505\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    506\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    507\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=384)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in docs:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a357be9d-ff4e-43e1-8046-735fa11de984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x7eadf8348810>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(collection_name='snappcloud',documents=docs, embedding=embeddings,persist_directory='./chromadb')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaf728a0-df4b-4088-9140-8be59d1a8d2d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "vectorstore = Qdrant.from_documents(documents=docs, embedding=embeddings, path='./quadrant',collection_name='snappcloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29048de",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = client.chat(model=llmModel, options = { 'temperature': 0}, messages=[{'role': 'system', 'content': systemPrompt},{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d664db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    print(\"using from context\",list(map(lambda x: x.metadata['source'],retrieved_docs)))\n",
    "    return ollama_llm(question, formatted_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a402c52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using from context ['docs/cache-proxy/cache-proxy.md', 'docs/message-queue/mq.md', 'docs/overview.md', 'docs/openstack/images.md']\n",
      "To use the Jaeger agent on SnappCloud, you should use the following address:\n",
      "\n",
      "`jaeger-agent:6832`\n",
      "\n",
      "This is according to the documentation provided in the `images` section of the SnappCloud documentation. The relevant document is not explicitly cited, but it appears to be a part of the overall SnappCloud documentation.\n",
      "\n",
      "Please note that this answer assumes that you are referring to the Jaeger agent for distributed tracing and monitoring. If you meant something else by \"Jaeger agent\", please clarify and I'll do my best to provide a more accurate response.\n",
      "CPU times: user 34.6 ms, sys: 6.1 ms, total: 40.7 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = rag_chain(\"What address should I use for jaeger agent on snappcloud?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "407b1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using from context ['docs/cache-proxy/cache-proxy.md', 'docs/ai-bigdata/kubeflow.md', 'docs/openstack/images.md', 'docs/networking/loadbalancer-as-a-service.md']\n",
      "To whitelist the external IPs for SnappCloud, you need to specify the following:\n",
      "\n",
      "* `172.16.10.10`\n",
      "* `172.16.10.11`\n",
      "* `172.16.10.12`\n",
      "\n",
      "These are the IP addresses that will be used as the endpoints for the ExternalService.\n",
      "\n",
      "You can also use the available images provided by SnappCloud, such as `centos7-public` or `ubuntu20-public`, to provision your instances and avoid preparing images yourself.\n",
      "\n",
      "Please note that LBaaS is in Alpha state, so it may not be fully supported yet.\n",
      "CPU times: user 16.4 ms, sys: 939 µs, total: 17.3 ms\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = rag_chain(\"What are the external IPs for snappcloud that i need to whitelist?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71551df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using from context ['docs/cache-proxy/cache-proxy.md', 'docs/ai-bigdata/kubeflow.md', 'docs/servicedesk.md', 'docs/message-queue/mq.md']\n",
      "To increase the quota for your project, you can follow these steps:\n",
      "\n",
      "1. Go to the Cloud Service Desk page.\n",
      "2. Click on \"Increase [S3/OpenStack/OKD] Quota\" to create a new ticket.\n",
      "3. Fill in the required fields:\n",
      "\t* Summary: Provide a concise description of your request and indicate whether the requested amount of resources is permanent or temporary.\n",
      "\t* [S3/OpenStack/OKD] User ID/Project Name/Team Name: Specify the relevant information for the service you are requesting resources for.\n",
      "\t* Cloud Region: Select the appropriate region for your project (teh-1 or teh-2).\n",
      "4. Optional additional fields:\n",
      "\t* Additional [resource type]: If you require an increase in a specific resource type (e.g., storage, memory, VCPUs), enter the desired amount in this field.\n",
      "\n",
      "Please note that filling out the additional fields is optional, and you do not need to complete all of them.\n",
      "\n",
      "Before creating a ticket, it's recommended to monitor your metrics to ensure that you are running low on resources. You can review the following metrics:\n",
      "\n",
      "* Storage Size\n",
      "* Bucket Count (S3)\n",
      "* Object Count (S3)\n",
      "* Memory\n",
      "* VCPUs\n",
      "\n",
      "By monitoring these metrics, you can make an informed decision about whether to proceed with creating a ticket for an increase in resources.\n",
      "\n",
      "Please also note that the High or Blocked priority should only be used in critical situations such as system downtime, outage, or when you are on the verge of failure. These priority levels are intended for urgent and severe cases that require immediate attention and resolution.\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain(\"I need to increase the quota for my project, how can I do this?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181e260-0be7-45ed-837d-ceb18f4de2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7db57-861c-4979-b863-293182394ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
